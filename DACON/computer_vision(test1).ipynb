{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "computer_vision(test1).ipynb",
      "provenance": [],
      "mount_file_id": "1xjozSiyMRALqQP923K2SD15Zadj1tbf3",
      "authorship_tag": "ABX9TyO6CG9kuQ1YW3Twbba+rdt9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "346a1bd1856c44ea88374b475b207a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d3dcd6568b894f9fbe6d447709a51c0e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_35d21a521c5d449182ae92a80dd236d0",
              "IPY_MODEL_e0fc55c49c7e4dc4961341291a6e96a3"
            ]
          }
        },
        "d3dcd6568b894f9fbe6d447709a51c0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35d21a521c5d449182ae92a80dd236d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_42d6958897df4b45b4e875c27c3db80e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae7d168df8314486884ea0a3ee21e910"
          }
        },
        "e0fc55c49c7e4dc4961341291a6e96a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5fb0a8487af045118bdcddf09b4527d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 75.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1993b987f3b84860ba71928da11a0b17"
          }
        },
        "42d6958897df4b45b4e875c27c3db80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae7d168df8314486884ea0a3ee21e910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fb0a8487af045118bdcddf09b4527d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1993b987f3b84860ba71928da11a0b17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jisoolee11/AI_study/blob/main/DACON/computer_vision(test1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAY54iJiq_-B"
      },
      "source": [
        "<h1> DACON 제 2회 컴퓨터 비전 학습 경진대회"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNvvmhU1aUzg",
        "outputId": "7f4df146-df4a-411c-96b9-510c2bde8397"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8WWj1XzbCZ3"
      },
      "source": [
        "!unzip -qq /content/gdrive/My\\ Drive/data_2/mnist_data.zip\r\n",
        "!unzip -qq /content/gdrive/MyDrive/data_2/test_dirty_mnist.zip\r\n",
        "!unzip -qq /content/gdrive/MyDrive/data_2/dirty_mnist.zip"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNdARf1Iclgc"
      },
      "source": [
        "import os\r\n",
        "from typing import Tuple, Sequence, Callable\r\n",
        "import csv\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from PIL import Image\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "from torch import nn, Tensor\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchsummary import summary\r\n",
        "\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision.models import resnet50"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp84hc0lcqr8"
      },
      "source": [
        "class MnistDataset(Dataset):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        dir: os.PathLike,\r\n",
        "        image_ids: os.PathLike,\r\n",
        "        transforms: Sequence[Callable]\r\n",
        "    ) -> None:\r\n",
        "        self.dir = dir\r\n",
        "        self.transforms = transforms\r\n",
        "\r\n",
        "        self.labels = {}\r\n",
        "        with open(image_ids, 'r') as f:\r\n",
        "            reader = csv.reader(f)\r\n",
        "            next(reader)\r\n",
        "            for row in reader:\r\n",
        "                self.labels[int(row[0])] = list(map(int, row[1:]))\r\n",
        "\r\n",
        "        self.image_ids = list(self.labels.keys())\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.image_ids)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int) -> Tuple[Tensor]:\r\n",
        "        image_id = self.image_ids[index]\r\n",
        "        image = Image.open(\r\n",
        "            os.path.join(\r\n",
        "                self.dir, f'{str(image_id).zfill(5)}.png')).convert('RGB')\r\n",
        "        target = np.array(self.labels.get(image_id)).astype(np.float32)\r\n",
        "\r\n",
        "        if self.transforms is not None:\r\n",
        "            image = self.transforms(image)\r\n",
        "\r\n",
        "        return image, target"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TmU691FcuCn"
      },
      "source": [
        "transforms_train = transforms.Compose([\r\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\r\n",
        "    transforms.RandomVerticalFlip(p=0.5),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "])\r\n",
        "\r\n",
        "transforms_test = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "867XDQnYcxup"
      },
      "source": [
        "trainset = MnistDataset('/content', '/content/gdrive/MyDrive/data_2/dirty_mnist_answer.csv', transforms_train)\r\n",
        "testset = MnistDataset('/content', '/content/gdrive/MyDrive/data_2/sample_submission.csv', transforms_test)\r\n",
        "\r\n",
        "train_loader = DataLoader(trainset, batch_size=32, num_workers=2)\r\n",
        "test_loader = DataLoader(testset, batch_size=10, num_workers=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "346a1bd1856c44ea88374b475b207a8d",
            "d3dcd6568b894f9fbe6d447709a51c0e",
            "35d21a521c5d449182ae92a80dd236d0",
            "e0fc55c49c7e4dc4961341291a6e96a3",
            "42d6958897df4b45b4e875c27c3db80e",
            "ae7d168df8314486884ea0a3ee21e910",
            "5fb0a8487af045118bdcddf09b4527d0",
            "1993b987f3b84860ba71928da11a0b17"
          ]
        },
        "id": "_GPLEvMEdHS6",
        "outputId": "97301857-be55-42af-a8df-5a55b6116c6a"
      },
      "source": [
        "class MnistModel(nn.Module):\r\n",
        "    def __init__(self) -> None:\r\n",
        "        super().__init__()\r\n",
        "        self.resnet = resnet50(pretrained=True)\r\n",
        "        self.classifier = nn.Linear(1000, 26)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.resnet(x)\r\n",
        "        x = self.classifier(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = MnistModel().to(device)\r\n",
        "print(summary(model,(3, 256, 256)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "346a1bd1856c44ea88374b475b207a8d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
            "              ReLU-3         [-1, 64, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
            "            Conv2d-5           [-1, 64, 64, 64]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
            "              ReLU-7           [-1, 64, 64, 64]               0\n",
            "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
            "             ReLU-10           [-1, 64, 64, 64]               0\n",
            "           Conv2d-11          [-1, 256, 64, 64]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 64, 64]             512\n",
            "           Conv2d-13          [-1, 256, 64, 64]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 64, 64]             512\n",
            "             ReLU-15          [-1, 256, 64, 64]               0\n",
            "       Bottleneck-16          [-1, 256, 64, 64]               0\n",
            "           Conv2d-17           [-1, 64, 64, 64]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 64, 64]             128\n",
            "             ReLU-19           [-1, 64, 64, 64]               0\n",
            "           Conv2d-20           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 64, 64]             128\n",
            "             ReLU-22           [-1, 64, 64, 64]               0\n",
            "           Conv2d-23          [-1, 256, 64, 64]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 64, 64]             512\n",
            "             ReLU-25          [-1, 256, 64, 64]               0\n",
            "       Bottleneck-26          [-1, 256, 64, 64]               0\n",
            "           Conv2d-27           [-1, 64, 64, 64]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 64, 64]             128\n",
            "             ReLU-29           [-1, 64, 64, 64]               0\n",
            "           Conv2d-30           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 64, 64]             128\n",
            "             ReLU-32           [-1, 64, 64, 64]               0\n",
            "           Conv2d-33          [-1, 256, 64, 64]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 64, 64]             512\n",
            "             ReLU-35          [-1, 256, 64, 64]               0\n",
            "       Bottleneck-36          [-1, 256, 64, 64]               0\n",
            "           Conv2d-37          [-1, 128, 64, 64]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 64, 64]             256\n",
            "             ReLU-39          [-1, 128, 64, 64]               0\n",
            "           Conv2d-40          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 32, 32]             256\n",
            "             ReLU-42          [-1, 128, 32, 32]               0\n",
            "           Conv2d-43          [-1, 512, 32, 32]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 32, 32]           1,024\n",
            "           Conv2d-45          [-1, 512, 32, 32]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-47          [-1, 512, 32, 32]               0\n",
            "       Bottleneck-48          [-1, 512, 32, 32]               0\n",
            "           Conv2d-49          [-1, 128, 32, 32]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 32, 32]             256\n",
            "             ReLU-51          [-1, 128, 32, 32]               0\n",
            "           Conv2d-52          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 32, 32]             256\n",
            "             ReLU-54          [-1, 128, 32, 32]               0\n",
            "           Conv2d-55          [-1, 512, 32, 32]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-57          [-1, 512, 32, 32]               0\n",
            "       Bottleneck-58          [-1, 512, 32, 32]               0\n",
            "           Conv2d-59          [-1, 128, 32, 32]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 32, 32]             256\n",
            "             ReLU-61          [-1, 128, 32, 32]               0\n",
            "           Conv2d-62          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 32, 32]             256\n",
            "             ReLU-64          [-1, 128, 32, 32]               0\n",
            "           Conv2d-65          [-1, 512, 32, 32]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-67          [-1, 512, 32, 32]               0\n",
            "       Bottleneck-68          [-1, 512, 32, 32]               0\n",
            "           Conv2d-69          [-1, 128, 32, 32]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 32, 32]             256\n",
            "             ReLU-71          [-1, 128, 32, 32]               0\n",
            "           Conv2d-72          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 32, 32]             256\n",
            "             ReLU-74          [-1, 128, 32, 32]               0\n",
            "           Conv2d-75          [-1, 512, 32, 32]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-77          [-1, 512, 32, 32]               0\n",
            "       Bottleneck-78          [-1, 512, 32, 32]               0\n",
            "           Conv2d-79          [-1, 256, 32, 32]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 32, 32]             512\n",
            "             ReLU-81          [-1, 256, 32, 32]               0\n",
            "           Conv2d-82          [-1, 256, 16, 16]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 16, 16]             512\n",
            "             ReLU-84          [-1, 256, 16, 16]               0\n",
            "           Conv2d-85         [-1, 1024, 16, 16]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 16, 16]           2,048\n",
            "           Conv2d-87         [-1, 1024, 16, 16]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 16, 16]           2,048\n",
            "             ReLU-89         [-1, 1024, 16, 16]               0\n",
            "       Bottleneck-90         [-1, 1024, 16, 16]               0\n",
            "           Conv2d-91          [-1, 256, 16, 16]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 16, 16]             512\n",
            "             ReLU-93          [-1, 256, 16, 16]               0\n",
            "           Conv2d-94          [-1, 256, 16, 16]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 16, 16]             512\n",
            "             ReLU-96          [-1, 256, 16, 16]               0\n",
            "           Conv2d-97         [-1, 1024, 16, 16]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 16, 16]           2,048\n",
            "             ReLU-99         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-100         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-101          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 16, 16]             512\n",
            "            ReLU-103          [-1, 256, 16, 16]               0\n",
            "          Conv2d-104          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 16, 16]             512\n",
            "            ReLU-106          [-1, 256, 16, 16]               0\n",
            "          Conv2d-107         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-109         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-110         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-111          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 16, 16]             512\n",
            "            ReLU-113          [-1, 256, 16, 16]               0\n",
            "          Conv2d-114          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 16, 16]             512\n",
            "            ReLU-116          [-1, 256, 16, 16]               0\n",
            "          Conv2d-117         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-119         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-120         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-121          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 16, 16]             512\n",
            "            ReLU-123          [-1, 256, 16, 16]               0\n",
            "          Conv2d-124          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 16, 16]             512\n",
            "            ReLU-126          [-1, 256, 16, 16]               0\n",
            "          Conv2d-127         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-129         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-130         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-131          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 16, 16]             512\n",
            "            ReLU-133          [-1, 256, 16, 16]               0\n",
            "          Conv2d-134          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 16, 16]             512\n",
            "            ReLU-136          [-1, 256, 16, 16]               0\n",
            "          Conv2d-137         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-139         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-140         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-141          [-1, 512, 16, 16]         524,288\n",
            "     BatchNorm2d-142          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-143          [-1, 512, 16, 16]               0\n",
            "          Conv2d-144            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-146            [-1, 512, 8, 8]               0\n",
            "          Conv2d-147           [-1, 2048, 8, 8]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 8, 8]           4,096\n",
            "          Conv2d-149           [-1, 2048, 8, 8]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 8, 8]           4,096\n",
            "            ReLU-151           [-1, 2048, 8, 8]               0\n",
            "      Bottleneck-152           [-1, 2048, 8, 8]               0\n",
            "          Conv2d-153            [-1, 512, 8, 8]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-155            [-1, 512, 8, 8]               0\n",
            "          Conv2d-156            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-158            [-1, 512, 8, 8]               0\n",
            "          Conv2d-159           [-1, 2048, 8, 8]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 8, 8]           4,096\n",
            "            ReLU-161           [-1, 2048, 8, 8]               0\n",
            "      Bottleneck-162           [-1, 2048, 8, 8]               0\n",
            "          Conv2d-163            [-1, 512, 8, 8]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-165            [-1, 512, 8, 8]               0\n",
            "          Conv2d-166            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-168            [-1, 512, 8, 8]               0\n",
            "          Conv2d-169           [-1, 2048, 8, 8]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 8, 8]           4,096\n",
            "            ReLU-171           [-1, 2048, 8, 8]               0\n",
            "      Bottleneck-172           [-1, 2048, 8, 8]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "          Linear-174                 [-1, 1000]       2,049,000\n",
            "          ResNet-175                 [-1, 1000]               0\n",
            "          Linear-176                   [-1, 26]          26,026\n",
            "================================================================\n",
            "Total params: 25,583,058\n",
            "Trainable params: 25,583,058\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 374.28\n",
            "Params size (MB): 97.59\n",
            "Estimated Total Size (MB): 472.62\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9by8o-KPdKbX",
        "outputId": "1c13fde7-68a9-4421-8f04-2b8efca2f6cf"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\r\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\r\n",
        "\r\n",
        "num_epochs = 5\r\n",
        "model.train()\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, targets) in enumerate(train_loader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        images = images.to(device)\r\n",
        "        targets = targets.to(device)\r\n",
        "\r\n",
        "        outputs = model(images)\r\n",
        "        loss = criterion(outputs, targets)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        if (i+1) % 10 == 0:\r\n",
        "            outputs = outputs > 0.5\r\n",
        "            acc = (outputs == targets).float().mean()\r\n",
        "            print(f'{epoch}: {loss.item():.5f}, {acc.item():.5f}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.70214, 0.54447\n",
            "0: 0.70570, 0.52043\n",
            "0: 0.71635, 0.52885\n",
            "0: 0.70091, 0.53365\n",
            "0: 0.69968, 0.52764\n",
            "0: 0.69889, 0.55168\n",
            "0: 0.69890, 0.54087\n",
            "0: 0.69700, 0.52404\n",
            "0: 0.69483, 0.53245\n",
            "0: 0.69598, 0.53245\n",
            "0: 0.70930, 0.51683\n",
            "0: 0.69159, 0.54688\n",
            "0: 0.69628, 0.53125\n",
            "0: 0.69599, 0.53726\n",
            "0: 0.69663, 0.52885\n",
            "0: 0.69316, 0.54207\n",
            "0: 0.69584, 0.52644\n",
            "0: 0.68876, 0.54207\n",
            "0: 0.69461, 0.52764\n",
            "0: 0.69209, 0.55168\n",
            "0: 0.69348, 0.53125\n",
            "0: 0.69098, 0.54688\n",
            "0: 0.69012, 0.53005\n",
            "0: 0.69332, 0.53966\n",
            "0: 0.69081, 0.54688\n",
            "0: 0.69268, 0.53726\n",
            "0: 0.68984, 0.54087\n",
            "0: 0.68520, 0.55409\n",
            "0: 0.68426, 0.53486\n",
            "0: 0.68710, 0.55288\n",
            "0: 0.69062, 0.53606\n",
            "0: 0.68105, 0.54688\n",
            "0: 0.68899, 0.53005\n",
            "0: 0.68612, 0.52404\n",
            "0: 0.69211, 0.53125\n",
            "0: 0.69060, 0.54447\n",
            "0: 0.69140, 0.52644\n",
            "0: 0.68627, 0.53726\n",
            "0: 0.68886, 0.54447\n",
            "0: 0.68424, 0.55649\n",
            "0: 0.68396, 0.53245\n",
            "0: 0.68910, 0.51683\n",
            "0: 0.68606, 0.53365\n",
            "0: 0.68611, 0.54928\n",
            "0: 0.68394, 0.54327\n",
            "0: 0.68357, 0.54327\n",
            "0: 0.68353, 0.53606\n",
            "0: 0.68627, 0.54447\n",
            "0: 0.68791, 0.53486\n",
            "0: 0.68545, 0.53966\n",
            "0: 0.68740, 0.53726\n",
            "0: 0.68057, 0.56370\n",
            "0: 0.68316, 0.55529\n",
            "0: 0.68001, 0.54087\n",
            "0: 0.67921, 0.53245\n",
            "0: 0.68657, 0.53125\n",
            "0: 0.68735, 0.52764\n",
            "0: 0.68020, 0.53966\n",
            "0: 0.67474, 0.55409\n",
            "0: 0.68370, 0.53966\n",
            "0: 0.67448, 0.55168\n",
            "0: 0.67546, 0.54688\n",
            "0: 0.67514, 0.54447\n",
            "0: 0.67702, 0.56370\n",
            "0: 0.67909, 0.56370\n",
            "0: 0.68752, 0.53125\n",
            "0: 0.68509, 0.53125\n",
            "0: 0.68049, 0.56971\n",
            "0: 0.67573, 0.55168\n",
            "0: 0.67810, 0.54447\n",
            "0: 0.67299, 0.56010\n",
            "0: 0.67596, 0.55048\n",
            "0: 0.68130, 0.55649\n",
            "0: 0.67660, 0.53966\n",
            "0: 0.67620, 0.54808\n",
            "0: 0.68759, 0.54808\n",
            "0: 0.67548, 0.54928\n",
            "0: 0.67168, 0.55168\n",
            "0: 0.67546, 0.54688\n",
            "0: 0.68752, 0.52163\n",
            "0: 0.68311, 0.54087\n",
            "0: 0.67335, 0.55048\n",
            "0: 0.67676, 0.55649\n",
            "0: 0.67987, 0.54327\n",
            "0: 0.68034, 0.54567\n",
            "0: 0.67770, 0.55529\n",
            "0: 0.68302, 0.54567\n",
            "0: 0.68258, 0.52885\n",
            "0: 0.67948, 0.55409\n",
            "0: 0.67529, 0.56611\n",
            "0: 0.67592, 0.56611\n",
            "0: 0.67755, 0.52163\n",
            "0: 0.67012, 0.56370\n",
            "0: 0.67785, 0.55889\n",
            "0: 0.66715, 0.56010\n",
            "0: 0.68091, 0.53245\n",
            "0: 0.68046, 0.52885\n",
            "0: 0.66912, 0.57812\n",
            "0: 0.68031, 0.54688\n",
            "0: 0.67068, 0.56130\n",
            "0: 0.67037, 0.57572\n",
            "0: 0.68169, 0.54327\n",
            "0: 0.67182, 0.54808\n",
            "0: 0.66904, 0.56010\n",
            "0: 0.66282, 0.56490\n",
            "0: 0.67127, 0.56490\n",
            "0: 0.67063, 0.53966\n",
            "0: 0.66562, 0.56611\n",
            "0: 0.67132, 0.55769\n",
            "0: 0.66278, 0.56851\n",
            "0: 0.67531, 0.55288\n",
            "0: 0.65184, 0.58053\n",
            "0: 0.65458, 0.56731\n",
            "0: 0.66355, 0.55769\n",
            "0: 0.66013, 0.55409\n",
            "0: 0.65734, 0.56731\n",
            "0: 0.63860, 0.57091\n",
            "0: 0.66477, 0.56370\n",
            "0: 0.65549, 0.56010\n",
            "0: 0.66227, 0.57933\n",
            "0: 0.66214, 0.58053\n",
            "0: 0.65824, 0.57212\n",
            "0: 0.65305, 0.57572\n",
            "0: 0.65994, 0.56851\n",
            "0: 0.65167, 0.55649\n",
            "0: 0.67044, 0.54447\n",
            "0: 0.65719, 0.58053\n",
            "0: 0.64456, 0.58053\n",
            "0: 0.65706, 0.57452\n",
            "0: 0.65580, 0.58053\n",
            "0: 0.64552, 0.59375\n",
            "0: 0.65016, 0.57812\n",
            "0: 0.64253, 0.58894\n",
            "0: 0.65200, 0.57212\n",
            "0: 0.63770, 0.59375\n",
            "0: 0.64341, 0.57812\n",
            "0: 0.66574, 0.57332\n",
            "0: 0.64828, 0.59736\n",
            "0: 0.65736, 0.57332\n",
            "0: 0.66116, 0.58173\n",
            "0: 0.66583, 0.54808\n",
            "0: 0.66790, 0.56370\n",
            "0: 0.65777, 0.58053\n",
            "0: 0.64961, 0.57812\n",
            "0: 0.63192, 0.60817\n",
            "0: 0.64382, 0.56731\n",
            "0: 0.65655, 0.58534\n",
            "0: 0.64844, 0.58894\n",
            "0: 0.63267, 0.59615\n",
            "0: 0.63712, 0.61899\n",
            "0: 0.64225, 0.59615\n",
            "0: 0.65101, 0.56971\n",
            "0: 0.64072, 0.57692\n",
            "0: 0.65095, 0.59736\n",
            "0: 0.64751, 0.56971\n",
            "0: 0.64490, 0.59375\n",
            "1: 0.63946, 0.58654\n",
            "1: 0.64541, 0.58774\n",
            "1: 0.64584, 0.59255\n",
            "1: 0.62718, 0.60817\n",
            "1: 0.64145, 0.59014\n",
            "1: 0.64153, 0.58534\n",
            "1: 0.64629, 0.58534\n",
            "1: 0.64319, 0.58654\n",
            "1: 0.63478, 0.59255\n",
            "1: 0.63333, 0.60096\n",
            "1: 0.65601, 0.56490\n",
            "1: 0.63333, 0.59375\n",
            "1: 0.64594, 0.56971\n",
            "1: 0.62509, 0.59736\n",
            "1: 0.64664, 0.58053\n",
            "1: 0.64264, 0.60337\n",
            "1: 0.64674, 0.57212\n",
            "1: 0.64409, 0.59255\n",
            "1: 0.64542, 0.57091\n",
            "1: 0.63901, 0.62260\n",
            "1: 0.64283, 0.58053\n",
            "1: 0.61171, 0.61418\n",
            "1: 0.64594, 0.58173\n",
            "1: 0.63402, 0.60938\n",
            "1: 0.63073, 0.58894\n",
            "1: 0.62277, 0.59976\n",
            "1: 0.64363, 0.61899\n",
            "1: 0.62794, 0.60697\n",
            "1: 0.62260, 0.59736\n",
            "1: 0.65428, 0.59615\n",
            "1: 0.62458, 0.59615\n",
            "1: 0.62742, 0.60216\n",
            "1: 0.63202, 0.60096\n",
            "1: 0.62812, 0.57933\n",
            "1: 0.63385, 0.60337\n",
            "1: 0.63006, 0.61058\n",
            "1: 0.61206, 0.62620\n",
            "1: 0.64461, 0.58293\n",
            "1: 0.63575, 0.59495\n",
            "1: 0.63067, 0.61779\n",
            "1: 0.61876, 0.61659\n",
            "1: 0.64430, 0.59736\n",
            "1: 0.62383, 0.60817\n",
            "1: 0.62878, 0.61058\n",
            "1: 0.63999, 0.58534\n",
            "1: 0.63980, 0.59615\n",
            "1: 0.62254, 0.59495\n",
            "1: 0.62524, 0.59976\n",
            "1: 0.63032, 0.58654\n",
            "1: 0.60655, 0.62380\n",
            "1: 0.63333, 0.59255\n",
            "1: 0.63211, 0.61418\n",
            "1: 0.62659, 0.61899\n",
            "1: 0.61841, 0.61178\n",
            "1: 0.61617, 0.60577\n",
            "1: 0.61034, 0.63822\n",
            "1: 0.61522, 0.61538\n",
            "1: 0.62469, 0.60817\n",
            "1: 0.60996, 0.63822\n",
            "1: 0.63231, 0.60457\n",
            "1: 0.62668, 0.60697\n",
            "1: 0.63404, 0.60457\n",
            "1: 0.61174, 0.62139\n",
            "1: 0.62053, 0.61779\n",
            "1: 0.61571, 0.62981\n",
            "1: 0.61409, 0.60817\n",
            "1: 0.63651, 0.61058\n",
            "1: 0.62058, 0.61659\n",
            "1: 0.64092, 0.59615\n",
            "1: 0.60221, 0.62981\n",
            "1: 0.60177, 0.63101\n",
            "1: 0.61950, 0.60457\n",
            "1: 0.63894, 0.60577\n",
            "1: 0.62190, 0.61538\n",
            "1: 0.62863, 0.59736\n",
            "1: 0.63523, 0.60096\n",
            "1: 0.60109, 0.62500\n",
            "1: 0.60988, 0.63942\n",
            "1: 0.61277, 0.61178\n",
            "1: 0.62246, 0.61659\n",
            "1: 0.59871, 0.62139\n",
            "1: 0.60248, 0.62500\n",
            "1: 0.62172, 0.62019\n",
            "1: 0.62050, 0.61178\n",
            "1: 0.61150, 0.63582\n",
            "1: 0.61139, 0.61538\n",
            "1: 0.61789, 0.61779\n",
            "1: 0.63912, 0.59135\n",
            "1: 0.61246, 0.61178\n",
            "1: 0.60718, 0.62740\n",
            "1: 0.59933, 0.65505\n",
            "1: 0.61485, 0.60457\n",
            "1: 0.59330, 0.65144\n",
            "1: 0.59319, 0.62380\n",
            "1: 0.59916, 0.63101\n",
            "1: 0.60169, 0.60697\n",
            "1: 0.62894, 0.58894\n",
            "1: 0.61964, 0.62380\n",
            "1: 0.60755, 0.63101\n",
            "1: 0.62102, 0.62260\n",
            "1: 0.59876, 0.65986\n",
            "1: 0.60870, 0.63101\n",
            "1: 0.57976, 0.62620\n",
            "1: 0.61370, 0.62139\n",
            "1: 0.56316, 0.66466\n",
            "1: 0.60109, 0.65264\n",
            "1: 0.58010, 0.65385\n",
            "1: 0.62120, 0.62740\n",
            "1: 0.60471, 0.64784\n",
            "1: 0.59478, 0.65505\n",
            "1: 0.63052, 0.60817\n",
            "1: 0.57501, 0.66226\n",
            "1: 0.59654, 0.62981\n",
            "1: 0.58892, 0.64183\n",
            "1: 0.59236, 0.63942\n",
            "1: 0.59092, 0.63822\n",
            "1: 0.58912, 0.64062\n",
            "1: 0.57403, 0.64784\n",
            "1: 0.58847, 0.64062\n",
            "1: 0.59522, 0.64784\n",
            "1: 0.57382, 0.65024\n",
            "1: 0.58764, 0.65264\n",
            "1: 0.59711, 0.63582\n",
            "1: 0.59096, 0.63822\n",
            "1: 0.59066, 0.63341\n",
            "1: 0.58137, 0.64183\n",
            "1: 0.59771, 0.63582\n",
            "1: 0.56993, 0.65385\n",
            "1: 0.59985, 0.62620\n",
            "1: 0.60332, 0.64423\n",
            "1: 0.58123, 0.65625\n",
            "1: 0.58797, 0.65264\n",
            "1: 0.56317, 0.67668\n",
            "1: 0.57810, 0.64423\n",
            "1: 0.57432, 0.66226\n",
            "1: 0.56421, 0.66106\n",
            "1: 0.60543, 0.64423\n",
            "1: 0.58265, 0.64303\n",
            "1: 0.56634, 0.65024\n",
            "1: 0.59339, 0.64062\n",
            "1: 0.60208, 0.62380\n",
            "1: 0.58509, 0.64784\n",
            "1: 0.55001, 0.67308\n",
            "1: 0.59290, 0.64904\n",
            "1: 0.58588, 0.65024\n",
            "1: 0.58532, 0.63702\n",
            "1: 0.60077, 0.66947\n",
            "1: 0.58006, 0.65986\n",
            "1: 0.57167, 0.64784\n",
            "1: 0.56935, 0.66947\n",
            "1: 0.57007, 0.66707\n",
            "1: 0.58652, 0.63462\n",
            "1: 0.56492, 0.67067\n",
            "1: 0.59161, 0.64663\n",
            "1: 0.60440, 0.63462\n",
            "1: 0.59106, 0.64423\n",
            "2: 0.58799, 0.64303\n",
            "2: 0.57013, 0.65625\n",
            "2: 0.57205, 0.66346\n",
            "2: 0.53558, 0.69832\n",
            "2: 0.56444, 0.66106\n",
            "2: 0.58705, 0.67067\n",
            "2: 0.60738, 0.64062\n",
            "2: 0.57622, 0.65024\n",
            "2: 0.57192, 0.66226\n",
            "2: 0.55945, 0.68870\n",
            "2: 0.57466, 0.64904\n",
            "2: 0.54509, 0.68990\n",
            "2: 0.55986, 0.65986\n",
            "2: 0.56474, 0.66587\n",
            "2: 0.59497, 0.65505\n",
            "2: 0.57257, 0.66827\n",
            "2: 0.60575, 0.63942\n",
            "2: 0.56585, 0.68029\n",
            "2: 0.57758, 0.64663\n",
            "2: 0.57006, 0.67188\n",
            "2: 0.58052, 0.65385\n",
            "2: 0.54081, 0.68630\n",
            "2: 0.56015, 0.67067\n",
            "2: 0.58577, 0.66707\n",
            "2: 0.55273, 0.68149\n",
            "2: 0.55563, 0.66707\n",
            "2: 0.58524, 0.67428\n",
            "2: 0.55759, 0.67067\n",
            "2: 0.56662, 0.65505\n",
            "2: 0.58709, 0.65745\n",
            "2: 0.52546, 0.70312\n",
            "2: 0.55031, 0.67668\n",
            "2: 0.53536, 0.69952\n",
            "2: 0.55460, 0.67428\n",
            "2: 0.54306, 0.68269\n",
            "2: 0.55572, 0.68870\n",
            "2: 0.55095, 0.68870\n",
            "2: 0.55858, 0.66707\n",
            "2: 0.56494, 0.67668\n",
            "2: 0.54393, 0.68389\n",
            "2: 0.55381, 0.69111\n",
            "2: 0.57161, 0.65505\n",
            "2: 0.54334, 0.68870\n",
            "2: 0.55223, 0.68510\n",
            "2: 0.55998, 0.69111\n",
            "2: 0.55587, 0.68269\n",
            "2: 0.55125, 0.69712\n",
            "2: 0.54408, 0.68149\n",
            "2: 0.55957, 0.67668\n",
            "2: 0.53259, 0.70433\n",
            "2: 0.53753, 0.68870\n",
            "2: 0.54314, 0.70072\n",
            "2: 0.55655, 0.68389\n",
            "2: 0.57042, 0.66707\n",
            "2: 0.55914, 0.66226\n",
            "2: 0.55777, 0.68029\n",
            "2: 0.53780, 0.70192\n",
            "2: 0.54640, 0.69231\n",
            "2: 0.51127, 0.71755\n",
            "2: 0.53883, 0.69231\n",
            "2: 0.55361, 0.68990\n",
            "2: 0.54047, 0.70673\n",
            "2: 0.53617, 0.69832\n",
            "2: 0.55164, 0.70312\n",
            "2: 0.56512, 0.68870\n",
            "2: 0.51258, 0.71034\n",
            "2: 0.56735, 0.66947\n",
            "2: 0.55134, 0.68990\n",
            "2: 0.55689, 0.68149\n",
            "2: 0.51806, 0.71875\n",
            "2: 0.51194, 0.71394\n",
            "2: 0.54297, 0.69111\n",
            "2: 0.54367, 0.69111\n",
            "2: 0.53621, 0.69231\n",
            "2: 0.53848, 0.68870\n",
            "2: 0.57445, 0.67428\n",
            "2: 0.53476, 0.68149\n",
            "2: 0.52419, 0.69111\n",
            "2: 0.53573, 0.70673\n",
            "2: 0.56172, 0.66466\n",
            "2: 0.54080, 0.68389\n",
            "2: 0.53249, 0.69471\n",
            "2: 0.54569, 0.70072\n",
            "2: 0.53460, 0.67909\n",
            "2: 0.54308, 0.69832\n",
            "2: 0.56350, 0.69111\n",
            "2: 0.54195, 0.69832\n",
            "2: 0.58148, 0.65505\n",
            "2: 0.55333, 0.67909\n",
            "2: 0.54117, 0.68990\n",
            "2: 0.52646, 0.68630\n",
            "2: 0.57117, 0.64784\n",
            "2: 0.52678, 0.71514\n",
            "2: 0.52648, 0.70673\n",
            "2: 0.52327, 0.70312\n",
            "2: 0.53729, 0.67788\n",
            "2: 0.54669, 0.67308\n",
            "2: 0.52657, 0.70673\n",
            "2: 0.52498, 0.69712\n",
            "2: 0.54674, 0.70312\n",
            "2: 0.52239, 0.71635\n",
            "2: 0.49626, 0.70553\n",
            "2: 0.53409, 0.66707\n",
            "2: 0.55134, 0.67668\n",
            "2: 0.49518, 0.72476\n",
            "2: 0.55332, 0.68389\n",
            "2: 0.52584, 0.70793\n",
            "2: 0.53471, 0.70673\n",
            "2: 0.51760, 0.70913\n",
            "2: 0.54157, 0.70192\n",
            "2: 0.57387, 0.67788\n",
            "2: 0.49987, 0.71875\n",
            "2: 0.55031, 0.68990\n",
            "2: 0.55656, 0.69712\n",
            "2: 0.56957, 0.67548\n",
            "2: 0.51903, 0.71034\n",
            "2: 0.51700, 0.70793\n",
            "2: 0.50369, 0.71154\n",
            "2: 0.52468, 0.69952\n",
            "2: 0.53809, 0.70433\n",
            "2: 0.50474, 0.72476\n",
            "2: 0.51499, 0.68870\n",
            "2: 0.54183, 0.70192\n",
            "2: 0.50992, 0.71514\n",
            "2: 0.51015, 0.71635\n",
            "2: 0.53547, 0.67067\n",
            "2: 0.53385, 0.71514\n",
            "2: 0.48166, 0.71755\n",
            "2: 0.51982, 0.72837\n",
            "2: 0.50275, 0.71995\n",
            "2: 0.51013, 0.70913\n",
            "2: 0.50479, 0.74279\n",
            "2: 0.50511, 0.72236\n",
            "2: 0.51263, 0.70192\n",
            "2: 0.50153, 0.73438\n",
            "2: 0.50924, 0.71034\n",
            "2: 0.54365, 0.68990\n",
            "2: 0.51562, 0.70192\n",
            "2: 0.51386, 0.71875\n",
            "2: 0.51976, 0.71875\n",
            "2: 0.52223, 0.69832\n",
            "2: 0.52511, 0.70673\n",
            "2: 0.49226, 0.72236\n",
            "2: 0.52355, 0.70072\n",
            "2: 0.48912, 0.71875\n",
            "2: 0.52265, 0.69471\n",
            "2: 0.52278, 0.71154\n",
            "2: 0.47710, 0.74279\n",
            "2: 0.52504, 0.69952\n",
            "2: 0.48319, 0.74399\n",
            "2: 0.47551, 0.72957\n",
            "2: 0.49669, 0.71154\n",
            "2: 0.48532, 0.73438\n",
            "2: 0.52794, 0.70913\n",
            "2: 0.52820, 0.71154\n",
            "2: 0.51370, 0.71755\n",
            "3: 0.52321, 0.71514\n",
            "3: 0.49216, 0.73678\n",
            "3: 0.53221, 0.71034\n",
            "3: 0.47081, 0.73918\n",
            "3: 0.50167, 0.70913\n",
            "3: 0.53931, 0.71394\n",
            "3: 0.51476, 0.71514\n",
            "3: 0.49923, 0.75000\n",
            "3: 0.49486, 0.72837\n",
            "3: 0.48960, 0.72716\n",
            "3: 0.52083, 0.70072\n",
            "3: 0.48469, 0.72957\n",
            "3: 0.48525, 0.72957\n",
            "3: 0.49117, 0.73918\n",
            "3: 0.53066, 0.70913\n",
            "3: 0.50980, 0.71755\n",
            "3: 0.52885, 0.69712\n",
            "3: 0.48953, 0.74159\n",
            "3: 0.51736, 0.68029\n",
            "3: 0.49151, 0.72356\n",
            "3: 0.50902, 0.73678\n",
            "3: 0.50908, 0.72356\n",
            "3: 0.50512, 0.72837\n",
            "3: 0.50690, 0.73317\n",
            "3: 0.51693, 0.72476\n",
            "3: 0.47458, 0.72236\n",
            "3: 0.51814, 0.72837\n",
            "3: 0.47801, 0.75000\n",
            "3: 0.48955, 0.72115\n",
            "3: 0.48904, 0.75120\n",
            "3: 0.45654, 0.75240\n",
            "3: 0.45817, 0.76442\n",
            "3: 0.47193, 0.75120\n",
            "3: 0.47786, 0.75481\n",
            "3: 0.47836, 0.73197\n",
            "3: 0.47104, 0.75240\n",
            "3: 0.46706, 0.74519\n",
            "3: 0.49689, 0.72716\n",
            "3: 0.47909, 0.74519\n",
            "3: 0.49210, 0.74639\n",
            "3: 0.47178, 0.75601\n",
            "3: 0.51669, 0.72115\n",
            "3: 0.45769, 0.76202\n",
            "3: 0.47669, 0.75000\n",
            "3: 0.48929, 0.74760\n",
            "3: 0.46926, 0.74519\n",
            "3: 0.51293, 0.71995\n",
            "3: 0.48938, 0.73317\n",
            "3: 0.47824, 0.73438\n",
            "3: 0.45070, 0.77043\n",
            "3: 0.46135, 0.75000\n",
            "3: 0.47018, 0.77163\n",
            "3: 0.46422, 0.75601\n",
            "3: 0.46818, 0.75240\n",
            "3: 0.47672, 0.74279\n",
            "3: 0.48707, 0.74880\n",
            "3: 0.46600, 0.76683\n",
            "3: 0.44896, 0.77284\n",
            "3: 0.43110, 0.77163\n",
            "3: 0.48107, 0.73918\n",
            "3: 0.46191, 0.75601\n",
            "3: 0.44863, 0.77404\n",
            "3: 0.47693, 0.75120\n",
            "3: 0.43634, 0.79447\n",
            "3: 0.47270, 0.75841\n",
            "3: 0.44163, 0.77404\n",
            "3: 0.49643, 0.74639\n",
            "3: 0.49085, 0.75481\n",
            "3: 0.50098, 0.74279\n",
            "3: 0.44439, 0.79447\n",
            "3: 0.44635, 0.76803\n",
            "3: 0.45658, 0.75721\n",
            "3: 0.46959, 0.76562\n",
            "3: 0.43628, 0.76683\n",
            "3: 0.45173, 0.76442\n",
            "3: 0.49344, 0.73918\n",
            "3: 0.46716, 0.75601\n",
            "3: 0.44038, 0.76082\n",
            "3: 0.43367, 0.78726\n",
            "3: 0.51644, 0.71394\n",
            "3: 0.49352, 0.73558\n",
            "3: 0.44728, 0.78365\n",
            "3: 0.44768, 0.77644\n",
            "3: 0.44258, 0.77885\n",
            "3: 0.47622, 0.76803\n",
            "3: 0.47561, 0.73918\n",
            "3: 0.45880, 0.76803\n",
            "3: 0.50177, 0.73678\n",
            "3: 0.45258, 0.78125\n",
            "3: 0.43723, 0.77404\n",
            "3: 0.44083, 0.76683\n",
            "3: 0.48891, 0.73438\n",
            "3: 0.45867, 0.76322\n",
            "3: 0.44064, 0.76803\n",
            "3: 0.43492, 0.77043\n",
            "3: 0.44903, 0.75361\n",
            "3: 0.44054, 0.76803\n",
            "3: 0.41840, 0.78125\n",
            "3: 0.44027, 0.77284\n",
            "3: 0.47697, 0.76442\n",
            "3: 0.45136, 0.78606\n",
            "3: 0.41755, 0.78365\n",
            "3: 0.44266, 0.77163\n",
            "3: 0.47159, 0.74639\n",
            "3: 0.40915, 0.78726\n",
            "3: 0.46671, 0.77764\n",
            "3: 0.42760, 0.78125\n",
            "3: 0.44817, 0.78606\n",
            "3: 0.47712, 0.76683\n",
            "3: 0.42705, 0.79688\n",
            "3: 0.44717, 0.76322\n",
            "3: 0.43098, 0.78726\n",
            "3: 0.46805, 0.75841\n",
            "3: 0.46734, 0.75120\n",
            "3: 0.45473, 0.77163\n",
            "3: 0.42632, 0.79447\n",
            "3: 0.44156, 0.77404\n",
            "3: 0.43928, 0.77764\n",
            "3: 0.42901, 0.78486\n",
            "3: 0.44793, 0.76683\n",
            "3: 0.40344, 0.80048\n",
            "3: 0.42314, 0.78966\n",
            "3: 0.45720, 0.77885\n",
            "3: 0.38154, 0.80649\n",
            "3: 0.39454, 0.79808\n",
            "3: 0.45928, 0.76562\n",
            "3: 0.45282, 0.78005\n",
            "3: 0.40462, 0.78726\n",
            "3: 0.46024, 0.75601\n",
            "3: 0.40767, 0.79447\n",
            "3: 0.44633, 0.77644\n",
            "3: 0.40703, 0.81250\n",
            "3: 0.41547, 0.81130\n",
            "3: 0.38902, 0.81250\n",
            "3: 0.41708, 0.80048\n",
            "3: 0.43113, 0.77885\n",
            "3: 0.47189, 0.77764\n",
            "3: 0.43220, 0.77524\n",
            "3: 0.42149, 0.81250\n",
            "3: 0.42958, 0.77644\n",
            "3: 0.43659, 0.77885\n",
            "3: 0.42003, 0.77284\n",
            "3: 0.41170, 0.79808\n",
            "3: 0.42770, 0.79207\n",
            "3: 0.39611, 0.80529\n",
            "3: 0.42213, 0.79808\n",
            "3: 0.44073, 0.78486\n",
            "3: 0.41078, 0.79207\n",
            "3: 0.44041, 0.77284\n",
            "3: 0.40562, 0.81250\n",
            "3: 0.37655, 0.81971\n",
            "3: 0.39197, 0.80649\n",
            "3: 0.42318, 0.78486\n",
            "3: 0.42667, 0.78245\n",
            "3: 0.43012, 0.77163\n",
            "3: 0.38599, 0.81010\n",
            "4: 0.43692, 0.78726\n",
            "4: 0.41749, 0.79567\n",
            "4: 0.43307, 0.79447\n",
            "4: 0.35442, 0.83654\n",
            "4: 0.39197, 0.81971\n",
            "4: 0.42792, 0.80048\n",
            "4: 0.39688, 0.80409\n",
            "4: 0.41059, 0.79928\n",
            "4: 0.39524, 0.79808\n",
            "4: 0.40618, 0.77764\n",
            "4: 0.38200, 0.79688\n",
            "4: 0.37636, 0.82212\n",
            "4: 0.37958, 0.81851\n",
            "4: 0.42473, 0.80168\n",
            "4: 0.43609, 0.78606\n",
            "4: 0.39473, 0.80529\n",
            "4: 0.41766, 0.81130\n",
            "4: 0.39248, 0.81370\n",
            "4: 0.42823, 0.78005\n",
            "4: 0.39135, 0.81010\n",
            "4: 0.42125, 0.80649\n",
            "4: 0.42011, 0.80529\n",
            "4: 0.39409, 0.81490\n",
            "4: 0.38265, 0.82091\n",
            "4: 0.42859, 0.80048\n",
            "4: 0.37344, 0.81370\n",
            "4: 0.42329, 0.78486\n",
            "4: 0.39526, 0.78966\n",
            "4: 0.38951, 0.80168\n",
            "4: 0.42769, 0.79567\n",
            "4: 0.37031, 0.81250\n",
            "4: 0.38815, 0.81731\n",
            "4: 0.38475, 0.83053\n",
            "4: 0.37902, 0.81490\n",
            "4: 0.42977, 0.79207\n",
            "4: 0.36500, 0.82452\n",
            "4: 0.36621, 0.82332\n",
            "4: 0.41053, 0.79087\n",
            "4: 0.35746, 0.83173\n",
            "4: 0.37505, 0.82212\n",
            "4: 0.36077, 0.82332\n",
            "4: 0.36845, 0.82212\n",
            "4: 0.37227, 0.82091\n",
            "4: 0.42077, 0.80409\n",
            "4: 0.39600, 0.81971\n",
            "4: 0.36654, 0.82692\n",
            "4: 0.36915, 0.81731\n",
            "4: 0.40248, 0.81611\n",
            "4: 0.39089, 0.79688\n",
            "4: 0.36217, 0.82933\n",
            "4: 0.36044, 0.83293\n",
            "4: 0.35613, 0.82692\n",
            "4: 0.36684, 0.82813\n",
            "4: 0.33924, 0.84014\n",
            "4: 0.38969, 0.81010\n",
            "4: 0.36061, 0.83413\n",
            "4: 0.37330, 0.83413\n",
            "4: 0.34985, 0.84135\n",
            "4: 0.37329, 0.82212\n",
            "4: 0.36418, 0.82332\n",
            "4: 0.34741, 0.83654\n",
            "4: 0.36714, 0.82332\n",
            "4: 0.35478, 0.83413\n",
            "4: 0.32283, 0.86058\n",
            "4: 0.35448, 0.83413\n",
            "4: 0.35435, 0.83774\n",
            "4: 0.41460, 0.81490\n",
            "4: 0.38886, 0.82933\n",
            "4: 0.37441, 0.83053\n",
            "4: 0.32718, 0.84615\n",
            "4: 0.35380, 0.83534\n",
            "4: 0.34206, 0.83654\n",
            "4: 0.36519, 0.82452\n",
            "4: 0.31837, 0.86538\n",
            "4: 0.38042, 0.82692\n",
            "4: 0.38191, 0.82452\n",
            "4: 0.39312, 0.81370\n",
            "4: 0.34151, 0.83774\n",
            "4: 0.33694, 0.85216\n",
            "4: 0.38438, 0.81010\n",
            "4: 0.36745, 0.82933\n",
            "4: 0.35166, 0.83293\n",
            "4: 0.36006, 0.83053\n",
            "4: 0.32533, 0.85577\n",
            "4: 0.36931, 0.82452\n",
            "4: 0.35483, 0.83053\n",
            "4: 0.34939, 0.82572\n",
            "4: 0.40044, 0.81490\n",
            "4: 0.35902, 0.83774\n",
            "4: 0.35045, 0.82933\n",
            "4: 0.34370, 0.84255\n",
            "4: 0.39880, 0.79808\n",
            "4: 0.36467, 0.82933\n",
            "4: 0.34893, 0.84255\n",
            "4: 0.34018, 0.83534\n",
            "4: 0.35099, 0.82212\n",
            "4: 0.36017, 0.82813\n",
            "4: 0.32783, 0.85577\n",
            "4: 0.32683, 0.85096\n",
            "4: 0.34400, 0.84736\n",
            "4: 0.31652, 0.85938\n",
            "4: 0.32186, 0.84736\n",
            "4: 0.32506, 0.84255\n",
            "4: 0.35917, 0.81971\n",
            "4: 0.38003, 0.82452\n",
            "4: 0.36635, 0.82091\n",
            "4: 0.30865, 0.85938\n",
            "4: 0.36849, 0.83894\n",
            "4: 0.37399, 0.83413\n",
            "4: 0.33095, 0.85817\n",
            "4: 0.35662, 0.84014\n",
            "4: 0.35534, 0.83534\n",
            "4: 0.37631, 0.83053\n",
            "4: 0.35480, 0.83894\n",
            "4: 0.38208, 0.82933\n",
            "4: 0.34746, 0.83654\n",
            "4: 0.30872, 0.85457\n",
            "4: 0.32301, 0.83774\n",
            "4: 0.31431, 0.85216\n",
            "4: 0.35962, 0.84375\n",
            "4: 0.33049, 0.84736\n",
            "4: 0.28562, 0.86298\n",
            "4: 0.35635, 0.83053\n",
            "4: 0.30737, 0.85577\n",
            "4: 0.32543, 0.85577\n",
            "4: 0.33545, 0.84255\n",
            "4: 0.36121, 0.82813\n",
            "4: 0.32410, 0.84255\n",
            "4: 0.32991, 0.83413\n",
            "4: 0.30412, 0.86178\n",
            "4: 0.31412, 0.85577\n",
            "4: 0.29232, 0.86298\n",
            "4: 0.32019, 0.85216\n",
            "4: 0.30053, 0.87500\n",
            "4: 0.35942, 0.83413\n",
            "4: 0.32753, 0.84255\n",
            "4: 0.33862, 0.84736\n",
            "4: 0.31035, 0.86298\n",
            "4: 0.30847, 0.86779\n",
            "4: 0.32809, 0.85577\n",
            "4: 0.33268, 0.83413\n",
            "4: 0.30292, 0.85096\n",
            "4: 0.30864, 0.85938\n",
            "4: 0.29158, 0.87260\n",
            "4: 0.31180, 0.86058\n",
            "4: 0.31428, 0.85817\n",
            "4: 0.34180, 0.84976\n",
            "4: 0.31051, 0.87260\n",
            "4: 0.33662, 0.85216\n",
            "4: 0.30636, 0.85938\n",
            "4: 0.27319, 0.88221\n",
            "4: 0.27596, 0.87861\n",
            "4: 0.32165, 0.83894\n",
            "4: 0.29004, 0.84135\n",
            "4: 0.29294, 0.85817\n",
            "4: 0.29996, 0.85817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PsS8EBHannW3",
        "outputId": "4e4b3118-336b-4d91-f6e5-8025d32afef0"
      },
      "source": [
        "submit = pd.read_csv('/content/gdrive/MyDrive/data_2/sample_submission.csv')\r\n",
        "\r\n",
        "model.eval()\r\n",
        "batch_size = test_loader.batch_size\r\n",
        "batch_index = 0\r\n",
        "for i, (images, targets) in enumerate(test_loader):\r\n",
        "    images = images.to(device)\r\n",
        "    targets = targets.to(device)\r\n",
        "    outputs = model(images)\r\n",
        "    outputs = outputs > 0.5\r\n",
        "    batch_index = i * batch_size\r\n",
        "    submit.iloc[batch_index:batch_index+batch_size, 1:] = \\\r\n",
        "        outputs.long().squeeze(0).detach().cpu().numpy()\r\n",
        "    \r\n",
        "submit.to_csv('submit1.csv', index=False)\r\n",
        "from google.colab import files\r\n",
        "files.download('submit1.csv')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ca9cc7cf-4c4a-4bf2-b492-b112a9a322c9\", \"submit1.csv\", 290058)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}